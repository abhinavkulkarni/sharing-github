{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TextGenerationPipeline\n",
    "from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model_dir = \"facebook/opt-2.7b\"\n",
    "quantized_model_dir = \"facebook/opt-2.7b-4bit-128g\"\n",
    "\n",
    "quantize_config = BaseQuantizeConfig(\n",
    "    bits=4,  # quantize model to 4-bit\n",
    "    group_size=128,  # it is recommended to set the value to 128\n",
    ")\n",
    "\n",
    "max_memory={0: \"11GiB\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon May 15 03:53:17 2023       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 530.30.02              Driver Version: 530.30.02    CUDA Version: 12.1     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                  Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf            Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3060         On | 00000000:01:00.0 Off |                  N/A |\n",
      "| 44%   51C    P8               20W / 170W|      3MiB / 12288MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_dir, use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "\n",
    "text = \"\"\"Yaccarino was earlier the advertising chief for Comcast Corp's NBCUniversal\n",
    "\n",
    "Newly appointed Twitter CEO Linda Yaccarino said in a tweet on Saturday that she has been inspired by owner Elon Musk's vision to create a brighter future and is excited to help to transform the social media platform.\n",
    "\n",
    "It was the first time Yaccarino has spoken publicly since the news broke Thursday that she was in talks to become the next CEO of Twitter. Musk, who has served as CEO since his $44 billion buyout of Twitter last October, announced her appointment on Friday.\n",
    "\n",
    "\"I've long been inspired by [Musk's] vision to create a brighter future. I'm excited to help bring this vision to Twitter and transform this business together!\" Yaccarino tweeted.\n",
    "\n",
    "    Thank you @elonmusk!\n",
    "\n",
    "    I've long been inspired by your vision to create a brighter future. I'm excited to help bring this vision to Twitter and transform this business together! https://t.co/BcvySu7K76\n",
    "    â€” Linda Yaccarino (@lindayacc) May 13, 2023\n",
    "\n",
    "Yaccarino, who as advertising chief for Comcast Corp's NBCUniversal spent several years modernising its ad business, said she is committed to Twitter's future, and said user feedback is vital to build Twitter 2.0.\n",
    "\n",
    "Yaccarino will take over a social media platform that has been trying to reverse a plunge in ad revenue and is beset with challenges, along with a heavy debt load.\n",
    "\n",
    "Since Musk acquired Twitter, advertisers have fled the platform, worried that their ads could appear next to inappropriate content after the company lost nearly 80% of staff. Musk earlier this year acknowledged that Twitter had suffered a massive decline in ad revenue.\n",
    "\n",
    "While Musk said Yaccarino would help build an \"everything app,\" which he has previously said could offer a variety of services such as peer-to-peer payments, his selection of an advertising veteran signalled that digital ads would continue to be a core focus of the business.\n",
    "\n",
    "Musk has long said he intended to find a new leader for Twitter.\n",
    "\n",
    "Musk, who is also the CEO of electric-vehicle maker Tesla Inc , on Friday said that bringing Yaccarino on as Twitter's new chief will help him devote more time to Tesla.\n",
    "\n",
    "(This story has not been edited by NDTV staff and is auto-generated from a syndicated feed.)\"\"\"\n",
    "\n",
    "doc = nlp(text)\n",
    "\n",
    "examples = [tokenizer(sent.text) for sent in doc.sents]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/autogptq/lib/python3.9/site-packages/torch/cuda/memory.py:303: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum GPU memory used: 6.59 GB; time taken: 284.97s\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(quantized_model_dir):\n",
    "    start = time.time()\n",
    "\n",
    "    # Start tracking GPU memory usage\n",
    "    torch.cuda.reset_max_memory_allocated()\n",
    "\n",
    "    # load un-quantized model, by default, the model will always be loaded into CPU memory\n",
    "    model = AutoGPTQForCausalLM.from_pretrained(pretrained_model_dir, quantize_config, max_memory=max_memory)\n",
    "\n",
    "    # quantize model, the examples should be list of dict whose keys can only be \"input_ids\" and \"attention_mask\"\n",
    "    model.quantize(examples, use_triton=False)\n",
    "\n",
    "    # save quantized model\n",
    "    model.save_quantized(quantized_model_dir)\n",
    "\n",
    "    # save quantized model using safetensors\n",
    "    model.save_quantized(quantized_model_dir, use_safetensors=True)\n",
    "\n",
    "    # Get the maximum GPU memory allocated\n",
    "    max_memory_used = torch.cuda.max_memory_allocated(device) / 1024**3  # Convert to gigabytes\n",
    "\n",
    "    end = time.time()\n",
    "\n",
    "    print(f\"Maximum GPU memory used: {max_memory_used:.2f} GB; time taken: {end-start:.2f}s\")\n",
    "\n",
    "    # Clear GPU memory\n",
    "    model.cpu()\n",
    "    del model\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "from auto_gptq.eval_tasks import SequenceClassificationTask\n",
    "from functools import partial\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset tweet_sentiment_multilingual (/home/abhinav/.cache/huggingface/datasets/cardiffnlp___tweet_sentiment_multilingual/english/0.1.0/936afd3cde120393429606f681b3b48d526873c45114068973f71e296ce80605)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "961b588c0dce47fd94338ff233929a7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f80eefd4ef8a4cd28d45439d7bfa7d20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/870 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "DATASET = \"cardiffnlp/tweet_sentiment_multilingual\"\n",
    "TEMPLATE = \"Question:What's the sentiment of the given text? Choices are {labels}.\\nText: {text}\\nAnswer:\"\n",
    "ID2LABEL = {\n",
    "    0: \"negative\",\n",
    "    1: \"neutral\",\n",
    "    2: \"positive\"\n",
    "}\n",
    "LABELS = list(ID2LABEL.values())\n",
    "\n",
    "\n",
    "def ds_refactor_fn(samples):\n",
    "    text_data = samples[\"text\"]\n",
    "    label_data = samples[\"label\"]\n",
    "\n",
    "    new_samples = {\"prompt\": [], \"label\": []}\n",
    "    for text, label in zip(text_data, label_data):\n",
    "        prompt = TEMPLATE.format(labels=LABELS, text=text)\n",
    "        new_samples[\"prompt\"].append(prompt)\n",
    "        new_samples[\"label\"].append(ID2LABEL[label])\n",
    "\n",
    "    return new_samples\n",
    "\n",
    "\n",
    "class IdentityModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(IdentityModel, self).__init__()\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "    \n",
    "\n",
    "task = SequenceClassificationTask(\n",
    "        model=IdentityModel(),\n",
    "        tokenizer=tokenizer,\n",
    "        classes=LABELS,\n",
    "        data_name_or_path=DATASET,\n",
    "        prompt_col_name=\"prompt\",\n",
    "        label_col_name=\"label\",\n",
    "        **{\n",
    "            \"num_samples\": 1000,  # how many samples will be sampled to evaluation\n",
    "            \"sample_max_len\": 1024,  # max tokens for each sample\n",
    "            \"block_max_len\": 2048,  # max tokens for each data block\n",
    "            # function to load dataset, one must only accept data_name_or_path as input \n",
    "            # and return datasets.Dataset\n",
    "            \"load_fn\": partial(datasets.load_dataset, name=\"english\"),  \n",
    "            # function to preprocess dataset, which is used for datasets.Dataset.map, \n",
    "            # must return Dict[str, list] with only two keys: [prompt_col_name, label_col_name]\n",
    "            \"preprocess_fn\": ds_refactor_fn,  \n",
    "            # truncate label when sample's length exceed sample_max_len\n",
    "            \"truncate_prompt\": False  \n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum GPU memory used: 6.55 GB; eval result for AutoModelForCausalLM facebook/opt-2.7b model: {'acc': 0.3528735632183908}; time taken: 23.46s\n",
      "Maximum GPU memory used: 6.79 GB; eval result for AutoGPTQForCausalLM facebook/opt-2.7b model: {'acc': 0.3563218390804598}; time taken: 37.36s\n",
      "Maximum GPU memory used: 3.08 GB; eval result for AutoGPTQForCausalLM facebook/opt-2.7b-4bit-128g model: {'acc': 0.32068965517241377}; time taken: 76.30s\n"
     ]
    }
   ],
   "source": [
    "model_fns = [\n",
    "    lambda: AutoModelForCausalLM.from_pretrained(pretrained_model_dir, torch_dtype=torch.float16),\n",
    "    lambda: AutoGPTQForCausalLM.from_pretrained(pretrained_model_dir, BaseQuantizeConfig(), max_memory=max_memory),\n",
    "    lambda: AutoGPTQForCausalLM.from_quantized(quantized_model_dir, use_triton=False, max_memory=max_memory)\n",
    "]\n",
    "\n",
    "model_types = [\n",
    "    f\"AutoModelForCausalLM {pretrained_model_dir}\",\n",
    "    f\"AutoGPTQForCausalLM {pretrained_model_dir}\",\n",
    "    f\"AutoGPTQForCausalLM {quantized_model_dir}\"\n",
    "]\n",
    "\n",
    "for model_fn, model_type in zip(model_fns, model_types):\n",
    "    # Start tracking GPU memory usage\n",
    "    torch.cuda.reset_max_memory_allocated()\n",
    "\n",
    "    # Load the model\n",
    "    model = model_fn()\n",
    "    model.to(device)\n",
    "\n",
    "    # Run the evaluation task\n",
    "    task.model = model\n",
    "    start = time.time()\n",
    "    result = task.run()\n",
    "    end = time.time()\n",
    "\n",
    "    # Get the maximum GPU memory allocated\n",
    "    max_memory_used = torch.cuda.max_memory_allocated(device) / 1024**3  # Convert to gigabytes\n",
    "\n",
    "    print(f\"Maximum GPU memory used: {max_memory_used:.2f} GB; eval result for {model_type} model: {result}; time taken: {time.time()-start:.2f}s\")\n",
    "\n",
    "    task.model = None\n",
    "    model.cpu()\n",
    "    del model\n",
    "    torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autogptq",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
